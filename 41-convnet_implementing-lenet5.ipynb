{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, Activation, MaxPooling2D, Dropout, AveragePooling2D\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack tuple\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resizing the MNIST digits \n",
    "LeNet-5 architecture requires 32x32 size images. Therfore, 28x28 images should be resized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refernece: https://github.com/astorfi/TensorFlow-World/tree/master/docs/tutorials/3-neural_network/autoencoder\n",
    "import numpy as np\n",
    "from skimage import transform\n",
    "\n",
    "def resize_batch(imgs):\n",
    "    # A function to resize a batch of MNIST images to (32, 32)\n",
    "    # Args:\n",
    "    #   imgs: a numpy array of size [batch_size, 28 X 28].\n",
    "    # Returns:\n",
    "    #   a numpy array of size [batch_size, 32, 32].\n",
    "    imgs = imgs.reshape((-1, 28, 28, 1))\n",
    "    resized_imgs = np.zeros((imgs.shape[0], 32, 32, 1))\n",
    "    for i in range(imgs.shape[0]):\n",
    "        resized_imgs[i, ..., 0] = transform.resize(imgs[i, ..., 0], (32, 32))\n",
    "    return resized_imgs\n",
    "\n",
    "x_train32 = np.zeros((60000, 32, 32))\n",
    "for i in range(60000):\n",
    "    x_train32[i] = resize_batch(x_train[i]).reshape(32,32)\n",
    "\n",
    "x_test32 = np.zeros((10000, 32, 32))\n",
    "\n",
    "for i in range(10000):\n",
    "    x_test32[i] = resize_batch(x_test[i]).reshape(32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape\n",
    "x_train = np.expand_dims(x_train32, axis=3)\n",
    "x_test = np.expand_dims(x_test32, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = np.load('x_train.npy')\n",
    "# x_test = np.load('x_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "Rewrite Lenet-5 Neural Network\n",
    "![Lenet](images/lenet5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    \n",
    "    # Convolution 1\n",
    "    Conv2D(filters = 6, kernel_size= (5,5) , input_shape=(32, 32, 1), strides=(1,1), activation='tanh'),\n",
    "    \n",
    "    # Average Pooling 1\n",
    "    AveragePooling2D(pool_size=(2,2), strides = (2,2)),\n",
    "    \n",
    "    # Convolution 2\n",
    "    Conv2D(filters = 16, kernel_size= (5,5), strides=(1,1), activation='tanh'),\n",
    "    \n",
    "    # Average Pooling 2\n",
    "    AveragePooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "        \n",
    "    Flatten(),\n",
    "    Dense(84, activation = 'tanh'),\n",
    "    Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = 'adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 50s 826us/step - loss: 0.4934 - accuracy: 0.8468\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 62s 1ms/step - loss: 0.2120 - accuracy: 0.9356\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 52s 871us/step - loss: 0.1593 - accuracy: 0.9517\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 58s 969us/step - loss: 0.1309 - accuracy: 0.9602\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 58s 974us/step - loss: 0.1142 - accuracy: 0.9657\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 58s 975us/step - loss: 0.1024 - accuracy: 0.9685\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 52s 866us/step - loss: 0.0928 - accuracy: 0.9721\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 52s 866us/step - loss: 0.0856 - accuracy: 0.9734\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 52s 863us/step - loss: 0.0781 - accuracy: 0.9757\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 56s 940us/step - loss: 0.0734 - accuracy: 0.9769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x64617ced0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 397us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09078305727560074, 0.9725000262260437]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
